{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrar por Categorias Específicas Utilize as etiquetas gramaticais para identificar:\n",
    "\n",
    "- Pronomes: Etiquetas como `PRP` (pronome pessoal).\n",
    "- Verbos de Ligação: Verbos como is, am (etiquetas: `VBZ`, `VBP`).\n",
    "- Adjetivos: Etiqueta `JJ`.\n",
    "- Referências de Tempo: Dependerá do contexto, mas geralmente são `NN` ou `RB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "# Baixa e inicializa o pipeline para o inglês\n",
    "#stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Everybody writes. Especially in academia. Students write and professors\\nwrite. And nonfiction writers, who are the third group of people this book is\\naiming to help, obviously write as well. And writing doesn’t necessarily\\nmean papers, articles or books, but everyday, basic writing. We write when\\nwe need to remember something, be it an idea, a quote or the outcome of a\\nstudy. We write when we want to organise our thoughts and when we want to\\nexchange ideas with others. Students write when they take an exam, but the\\nfirst thing they do to prepare even for an oral examination is to grab pen and\\npaper. We write down not only those things we fear we won’t remember\\notherwise, but also the very things we try to memorise. Every intellectual\\nendeavour starts with a note.\\nWriting plays such a central role in learning, studying and research that it\\nis surprising how little we think about it. If writing is discussed, the focus lies\\nalmost always on the few exceptional moments where we write a lengthy\\npiece, a book, an article or, as students, the essays and theses we have to hand\\nin. At first glance, that makes sense: these are the tasks that cause the most\\nanxiety and with which we struggle the longest. Consequently, these “written\\npieces” are also what most self-help books for academics or study guides\\nfocus on, but very few give guidance for the everyday note-taking that takes\\nup the biggest chunk of our writing.\\nThe available books fall roughly into two categories. The first teaches the\\nformal requirements: style, structure or how to quote correctly. And then\\nthere are the psychological ones, which teach you how to get it done without\\nmental breakdowns and before your supervisor or publisher starts refusing to\\nmove the deadline once more. What they all have in common, though, is that\\nthey start with a blank screen or sheet of paper.\\n[1] But by doing this, they\\nignore the main part, namely note-taking, failing to understand that\\nimproving the organisation of all writing makes a difference. They seem to\\nforget that the process of writing starts much, much earlier than that blank\\nscreen and that the actual writing down of the argument is the smallest part of\\nits development. This book aims to fill this gap by showing you how to\\n'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/guilherme/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/guilherme/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Tokenizador\n",
    "nltk.download('averaged_perceptron_tagger')  # Modelo de POS tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'white if hd great surrounded coming hey hurt never boy caps us on left stink dark to are clone rich over none fuckin lowkey own everyday is weed verses you see right so ki harveys really keep honestly dreams veins done this hell believe ever haiku thirteen and watch fake chirp upgrade kid cause they could mean phones sun lot am weak of japayne stoned by reek bad biggity jerk best emma hate all young smoke microscope from peeled get come real face step that nwas about way have new who yup mood gettin asians aight police thang something these motherfuckin sleep learned spell wake my out school investigations outside high a nigga aye some he till ya i god better bang its day else three will cavalier did go correct needs your rappers just it getting well bar one puts spots cannot feel two how daughter soap shots test shit look say beat nose since hydroplane think but thinks dumb do drake niggas like telescope teach the contest play me what kyle even there change sound shade lane doing nerf up cd not lie know grab swear package been for with uhh boyz got in tell girls threw range killed be same d fuck working was filled saying straight millionaires someone dude bet let city no'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "text = texto\n",
    "\n",
    "# Função para expandir contrações no texto\n",
    "def expand_contractions(text, contraction_map):\n",
    "    \"\"\"\n",
    "    # Expande contrações\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    expanded_words = [contraction_map[word.lower()] if word.lower() in contraction_map else word for word in words]\n",
    "    return ' '.join(expanded_words)\n",
    "\n",
    "expanded_words = expand_contractions(text, contraction_map)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "punctuation = r'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "translator = str.maketrans('', '', punctuation)\n",
    "cleaned_text = text.translate(translator)\n",
    "\n",
    "cleaned_text = cleaned_text.lower()\n",
    "tokens = cleaned_text.split(\" \")\n",
    "\n",
    "tokens = set(tokens)\n",
    "\n",
    "text = \" \".join(tokens)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pronomes: ['us', 'none', 'you', 'they', 'who', 'something', 'my', 'he', 'i', 'its', 'your', 'it', 'one', 'me', 'there', 'someone']\n",
      "Verbos de Ligação: ['hd', 'surrounded', 'coming', 'hurt', 'caps', 'are', 'is', 'see', 'keep', 'done', 'believe', 'watch', 'could', 'mean', 'am', 'stoned', 'hate', 'peeled', 'get', 'come', 'have', 'thang', 'learned', 'wake', 'god', 'bang', 'will', 'cavalier', 'did', 'go', 'needs', 'getting', 'puts', 'can', 'feel', 'test', 'look', 'say', 'beat', 'think', 'thinks', 'do', 'teach', 'play', 'change', 'doing', 'lie', 'know', 'been', 'got', 'tell', 'threw', 'killed', 'be', 'was', 'filled', 'saying', 'bet', 'let']\n",
      "Adjetivos: ['white', 'great', 'left', 'dark', 'rich', 'fuckin', 'own', 'fake', 'weak', 'bad', 'best', 'young', 'real', 'new', 'aight', 'high', 'better', 'else', 'correct', 'dumb', 'same', 'straight']\n",
      "Referências de Tempo: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Função para classificar palavras por categoria\n",
    "def classify_words(doc):\n",
    "    pronouns = []\n",
    "    verbs = []\n",
    "    adjectives = []\n",
    "    time_refs = []\n",
    "    list_others = []\n",
    "\n",
    "    time_reference_words = {'today', 'tomorrow', 'yesterday', 'before', 'after'}\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            # Categoriza palavras por tipo gramatical\n",
    "            if word.upos == \"PRON\":\n",
    "                pronouns.append(word.text)\n",
    "            elif word.upos in [\"AUX\", \"VERB\"]:\n",
    "                verbs.append(word.text)\n",
    "            elif word.upos == \"ADJ\":\n",
    "                adjectives.append(word.text)\n",
    "            elif word.text.lower() in time_reference_words:\n",
    "                time_refs.append(word.text)\n",
    "            else:\n",
    "                list_others.append({\"text\": word.text, \"upos\":word.upos})\n",
    "\n",
    "    return pronouns, verbs, adjectives, time_refs, list_others\n",
    "\n",
    "# Texto de exemplo\n",
    "\n",
    "\n",
    "\n",
    "# Processa o texto expandido\n",
    "doc = nlp(text)\n",
    "\n",
    "# Classifica palavras\n",
    "pronouns, verbs, adjectives, time_refs, list_others = classify_words(doc)\n",
    "\n",
    "# Resultados\n",
    "print(f\"Pronomes: {pronouns}\")\n",
    "print(f\"Verbos de Ligação: {verbs}\")\n",
    "print(f\"Adjetivos: {adjectives}\")\n",
    "print(f\"Referências de Tempo: {time_refs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They like exciting city tomorrow.\n",
      "They want interesting food in the morning.\n",
      "You like easy music yesterday.\n",
      "She see easy food today.\n",
      "She know easy music today.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Defina as listas de palavras que você quer usar\n",
    "pronouns = ['I', 'You', 'He', 'She', 'We', 'They']\n",
    "verbs = ['eat', 'like', 'see', 'know', 'play', 'want']\n",
    "adjectives = ['beautiful', 'delicious', 'interesting', 'exciting', 'easy', 'fun']\n",
    "nouns = ['food', 'movie', 'game', 'book', 'city', 'music']\n",
    "time_refs = ['today', 'tomorrow', 'yesterday', 'in the morning', 'at night']\n",
    "\n",
    "# Função para gerar uma frase com base nas listas de palavras\n",
    "def generate_sentence(pronouns, verbs, adjectives, nouns, time_refs):\n",
    "    # Escolhe aleatoriamente uma palavra de cada lista\n",
    "    pronoun = random.choice(pronouns)\n",
    "    verb = random.choice(verbs)\n",
    "    adjective = random.choice(adjectives)\n",
    "    noun = random.choice(nouns)\n",
    "    time_ref = random.choice(time_refs)\n",
    "\n",
    "    # Combina as palavras em uma estrutura de frase simples\n",
    "    sentence = f\"{pronoun} {verb} {adjective} {noun} {time_ref}.\"\n",
    "    return sentence\n",
    "\n",
    "# Gerar algumas frases\n",
    "for _ in range(5):\n",
    "    print(generate_sentence(pronouns, verbs, adjectives, nouns, time_refs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pronomes [PRP]: ['you', 'they', 'he', 'ya', 'it', 'me']\n",
      "Verbos de Ligação [VBZ, VBP]: ['are', 'is', 'see', 'believe', 'haiku', 'nwas', 'have', 'yup', 'god', 'needs', 'puts', 'test', 'say', 'think', 'thinks', 'do', 'teach', \"ain't\", 'change', 'know', \"harvey's\", 'dude']\n",
      "Adjetivos [JJ]: ['white', 'hd', 'great', 'left', 'rich', 'lowkey', 'own', 'weed', 'right', 'dreams', 'thirteen', 'watch', 'upgrade', 'weak', 'reek', 'bad', 'young', 'real', 'new', 'aight', 'sleep', 'high', 'nigga', 'correct', 'daughter', 'shit', 'nose', 'dumb', 'sound', \"gettin'\", \"didn't\", 'grab', 'swear', 'uhh', 'threw', 'same', 'd', 'straight']\n",
      "Referências de Tempo [NN, RB]: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def classify_words(tagged_tokens):\n",
    "    pronouns = [word for word, tag in tagged_tokens if tag in ['PRP']]\n",
    "    verbs = [word for word, tag in tagged_tokens if tag in ['VBZ', 'VBP']]\n",
    "    adjectives = [word for word, tag in tagged_tokens if tag == 'JJ']\n",
    "    time_refs = [word for word, tag in tagged_tokens if word.lower() in ['today', 'tomorrow', 'yesterday', 'before', 'after']]\n",
    "\n",
    "    return pronouns, verbs, adjectives, time_refs\n",
    "\n",
    "pronouns, verbs, adjectives, time_refs = classify_words(tagged_tokens)\n",
    "print(f\"Pronomes [PRP]: {pronouns}\")\n",
    "print(f\"Verbos de Ligação [VBZ, VBP]: {verbs}\")\n",
    "print(f\"Adjetivos [JJ]: {adjectives}\")\n",
    "print(f\"Referências de Tempo [NN, RB]: {time_refs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast-english-s-Sv58CE-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
